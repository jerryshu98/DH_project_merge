{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or '3' to suppress all messages\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "label_encoder = LabelEncoder()\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = './data/prive_24h_data.csv'\n",
    "flag_data_path = './data/ground_truth_11_21.csv'\n",
    "\n",
    "data_df = pd.read_csv(raw_data_path)\n",
    "flag_data_df = pd.read_csv(flag_data_path)\n",
    "data_df['gender'] = label_encoder.fit_transform(data_df['gender'])\n",
    "data_df['race'] = label_encoder.fit_transform(data_df['race'])\n",
    "data_df['admission_type_x'] = label_encoder.fit_transform(data_df['admission_type_x'])\n",
    "data_df['first_careunit'] = label_encoder.fit_transform(data_df['first_careunit'])\n",
    "data_df['admission_type_y'] = label_encoder.fit_transform(data_df['admission_type_y'])\n",
    "data_df['insurance'] = label_encoder.fit_transform(data_df['insurance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    if df.isna().any().any():\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2811, 24, 27)\n",
      "(2811, 1)\n"
     ]
    }
   ],
   "source": [
    "total_x = []\n",
    "total_y = []\n",
    "for index, row in flag_data_df.iterrows():\n",
    "    id_df = data_df[data_df['stay_id'] == row['stay_id']]\n",
    "    if not check_missing_values(id_df):\n",
    "        label = id_df['label'].iloc[0]\n",
    "        if label == 1:\n",
    "            label = -72\n",
    "        else:\n",
    "            if id_df['Rev_h'].iloc[0] != -1000:\n",
    "                label = 48 - id_df['Rev_h'].iloc[0]\n",
    "            elif id_df['dod_h'].iloc[0] != -1000 and id_df['dod_h'].iloc[0]>0 and id_df['dod_h'].iloc[0]<48:\n",
    "                label = 96 - id_df['dod_h'].iloc[0]*2\n",
    "            else:\n",
    "                label = 96\n",
    "        id_df = id_df.drop(columns='label')\n",
    "        id_df = id_df.drop(columns='charttime')\n",
    "        total_x.append(id_df.values)\n",
    "        total_y.append(label)\n",
    "\n",
    "total_x = np.array(total_x)\n",
    "total_y = np.array(total_y).reshape(-1, 1)\n",
    "\n",
    "print(total_x.shape)\n",
    "print(total_y.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_module(X_train, y_train, X_test, y_test, epoch, learning_rate, batch):\n",
    "\n",
    "    # Define model and train\n",
    "    def build_lstm_model(timesteps, num_features):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(128, input_shape=(timesteps, num_features)),\n",
    "            tf.keras.layers.Dense(1) \n",
    "        ])\n",
    "        model.build(input_shape=(None, timesteps, num_features))\n",
    "        return model\n",
    "\n",
    "    model = build_lstm_model(timesteps=X_train.shape[1], num_features=X_train.shape[2])\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), loss='mean_squared_error') \n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epoch, batch_size=batch, validation_data=(X_test, y_test))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 23:09:05.245721: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-11-21 23:09:05.355419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/71 [>.............................] - ETA: 1s - loss: 5130.2007 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 23:09:05.456398: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - ETA: 0s - loss: 4555.3398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 23:09:06.523825: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-11-21 23:09:06.564757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 2s 17ms/step - loss: 4555.3398 - val_loss: 4343.7388\n",
      "Epoch 2/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4348.4751 - val_loss: 4330.0430\n",
      "Epoch 3/200\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 4332.3496 - val_loss: 4332.8901\n",
      "Epoch 4/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4328.2979 - val_loss: 4336.4727\n",
      "Epoch 5/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4326.8911 - val_loss: 4336.5903\n",
      "Epoch 6/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4327.7271 - val_loss: 4335.6699\n",
      "Epoch 7/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4326.2812 - val_loss: 4337.7422\n",
      "Epoch 8/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4325.2466 - val_loss: 4335.4932\n",
      "Epoch 9/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4320.5322 - val_loss: 4315.4131\n",
      "Epoch 10/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4299.3926 - val_loss: 4274.5283\n",
      "Epoch 11/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4241.7217 - val_loss: 4166.4268\n",
      "Epoch 12/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4015.4692 - val_loss: 3968.6333\n",
      "Epoch 13/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3923.3669 - val_loss: 3794.9729\n",
      "Epoch 14/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3764.4409 - val_loss: 3718.7251\n",
      "Epoch 15/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3894.3550 - val_loss: 4607.1089\n",
      "Epoch 16/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4608.4253 - val_loss: 4454.5737\n",
      "Epoch 17/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4469.4277 - val_loss: 4371.3853\n",
      "Epoch 18/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4391.4521 - val_loss: 4339.0796\n",
      "Epoch 19/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4354.9131 - val_loss: 4314.8037\n",
      "Epoch 20/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4311.0669 - val_loss: 4273.8457\n",
      "Epoch 21/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4267.2856 - val_loss: 4218.1948\n",
      "Epoch 22/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4177.0371 - val_loss: 4087.0503\n",
      "Epoch 23/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4035.3708 - val_loss: 3985.6121\n",
      "Epoch 24/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3928.6267 - val_loss: 3892.4241\n",
      "Epoch 25/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3769.6587 - val_loss: 3748.7375\n",
      "Epoch 26/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3723.5645 - val_loss: 3770.4797\n",
      "Epoch 27/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3814.1531 - val_loss: 3875.6826\n",
      "Epoch 28/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3598.1025 - val_loss: 3685.0471\n",
      "Epoch 29/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3608.0859 - val_loss: 3632.0354\n",
      "Epoch 30/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3609.6729 - val_loss: 3652.8826\n",
      "Epoch 31/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3587.4138 - val_loss: 3788.1409\n",
      "Epoch 32/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3451.8816 - val_loss: 3392.3804\n",
      "Epoch 33/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3408.3462 - val_loss: 3503.1270\n",
      "Epoch 34/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3545.3997 - val_loss: 3616.3689\n",
      "Epoch 35/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4029.4741 - val_loss: 5158.9771\n",
      "Epoch 36/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 4633.4858 - val_loss: 3831.3232\n",
      "Epoch 37/200\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 3553.3125 - val_loss: 3809.3796\n",
      "Epoch 38/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3535.9062 - val_loss: 3497.4353\n",
      "Epoch 39/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3408.8584 - val_loss: 3649.2246\n",
      "Epoch 40/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3362.8213 - val_loss: 3424.9121\n",
      "Epoch 41/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3299.1282 - val_loss: 3552.3535\n",
      "Epoch 42/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3473.0483 - val_loss: 3432.1533\n",
      "Epoch 43/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3245.2910 - val_loss: 3389.2688\n",
      "Epoch 44/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3281.8164 - val_loss: 3382.1145\n",
      "Epoch 45/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3264.3696 - val_loss: 3323.0295\n",
      "Epoch 46/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3257.2100 - val_loss: 3406.5601\n",
      "Epoch 47/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3263.3435 - val_loss: 3304.7302\n",
      "Epoch 48/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3208.6238 - val_loss: 3318.2815\n",
      "Epoch 49/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3226.3110 - val_loss: 3295.8438\n",
      "Epoch 50/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3191.2070 - val_loss: 3408.3989\n",
      "Epoch 51/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3192.6250 - val_loss: 3555.2224\n",
      "Epoch 52/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3297.6763 - val_loss: 3324.8027\n",
      "Epoch 53/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3131.0881 - val_loss: 3271.0388\n",
      "Epoch 54/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3087.3215 - val_loss: 3262.5134\n",
      "Epoch 55/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3072.9294 - val_loss: 3260.9067\n",
      "Epoch 56/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3083.7832 - val_loss: 3293.4919\n",
      "Epoch 57/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3249.5535 - val_loss: 3281.0662\n",
      "Epoch 58/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3126.4351 - val_loss: 3353.4727\n",
      "Epoch 59/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3146.3159 - val_loss: 3618.7146\n",
      "Epoch 60/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3314.0452 - val_loss: 3507.5144\n",
      "Epoch 61/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3076.1470 - val_loss: 3254.0879\n",
      "Epoch 62/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3009.3542 - val_loss: 3287.1780\n",
      "Epoch 63/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3069.6934 - val_loss: 3842.2180\n",
      "Epoch 64/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3275.9792 - val_loss: 3269.9900\n",
      "Epoch 65/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3017.8066 - val_loss: 3223.0044\n",
      "Epoch 66/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3007.1660 - val_loss: 3271.8545\n",
      "Epoch 67/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2987.1877 - val_loss: 3261.0049\n",
      "Epoch 68/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3480.8850 - val_loss: 3425.7629\n",
      "Epoch 69/200\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 3089.8467 - val_loss: 3309.4338\n",
      "Epoch 70/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2954.5610 - val_loss: 3165.6887\n",
      "Epoch 71/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2972.9756 - val_loss: 3115.4175\n",
      "Epoch 72/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2921.2710 - val_loss: 3274.0627\n",
      "Epoch 73/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2944.5417 - val_loss: 3205.1133\n",
      "Epoch 74/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3027.6226 - val_loss: 3215.9165\n",
      "Epoch 75/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3001.1135 - val_loss: 3067.7393\n",
      "Epoch 76/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2996.4907 - val_loss: 3162.1235\n",
      "Epoch 77/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2885.4846 - val_loss: 3138.7612\n",
      "Epoch 78/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2887.5400 - val_loss: 3090.8826\n",
      "Epoch 79/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2831.2947 - val_loss: 3021.7893\n",
      "Epoch 80/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2968.4023 - val_loss: 3212.9597\n",
      "Epoch 81/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2882.6533 - val_loss: 3049.5725\n",
      "Epoch 82/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2837.0073 - val_loss: 2960.2471\n",
      "Epoch 83/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2782.6841 - val_loss: 3181.0918\n",
      "Epoch 84/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2750.5845 - val_loss: 2958.9451\n",
      "Epoch 85/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2849.4954 - val_loss: 3562.0850\n",
      "Epoch 86/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3728.4668 - val_loss: 3582.0952\n",
      "Epoch 87/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3288.5959 - val_loss: 3471.9556\n",
      "Epoch 88/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3002.9812 - val_loss: 3043.1094\n",
      "Epoch 89/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2801.0435 - val_loss: 2879.2866\n",
      "Epoch 90/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2721.6077 - val_loss: 2966.0525\n",
      "Epoch 91/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2744.8960 - val_loss: 3186.3125\n",
      "Epoch 92/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2752.1702 - val_loss: 2924.0991\n",
      "Epoch 93/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2718.6797 - val_loss: 2921.7661\n",
      "Epoch 94/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2666.9065 - val_loss: 2838.2595\n",
      "Epoch 95/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2601.6545 - val_loss: 2823.9907\n",
      "Epoch 96/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2621.3149 - val_loss: 2789.7341\n",
      "Epoch 97/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2600.0369 - val_loss: 2752.8218\n",
      "Epoch 98/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2774.9019 - val_loss: 2872.4431\n",
      "Epoch 99/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2625.2461 - val_loss: 3028.8225\n",
      "Epoch 100/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2875.2058 - val_loss: 3365.1428\n",
      "Epoch 101/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 3028.2625 - val_loss: 3191.9729\n",
      "Epoch 102/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2846.5508 - val_loss: 3125.2473\n",
      "Epoch 103/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2807.5676 - val_loss: 2998.9739\n",
      "Epoch 104/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2909.9958 - val_loss: 2966.7876\n",
      "Epoch 105/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2744.9888 - val_loss: 2829.6616\n",
      "Epoch 106/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2665.7749 - val_loss: 3073.8545\n",
      "Epoch 107/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2594.4221 - val_loss: 2818.7019\n",
      "Epoch 108/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2618.8708 - val_loss: 2774.2378\n",
      "Epoch 109/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2624.4736 - val_loss: 2997.5708\n",
      "Epoch 110/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2610.2490 - val_loss: 2824.3533\n",
      "Epoch 111/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2654.6472 - val_loss: 2922.5237\n",
      "Epoch 112/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2672.7800 - val_loss: 2853.3752\n",
      "Epoch 113/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2560.4641 - val_loss: 2688.7866\n",
      "Epoch 114/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2532.2710 - val_loss: 2838.7898\n",
      "Epoch 115/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2516.4329 - val_loss: 2726.0657\n",
      "Epoch 116/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2524.3989 - val_loss: 2656.2078\n",
      "Epoch 117/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2482.7043 - val_loss: 2624.8662\n",
      "Epoch 118/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2413.2153 - val_loss: 2631.6353\n",
      "Epoch 119/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2447.0750 - val_loss: 2696.1748\n",
      "Epoch 120/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2468.1604 - val_loss: 2821.0122\n",
      "Epoch 121/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2422.0818 - val_loss: 2651.9607\n",
      "Epoch 122/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2438.9639 - val_loss: 2658.5305\n",
      "Epoch 123/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2394.1543 - val_loss: 2596.0439\n",
      "Epoch 124/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2465.9119 - val_loss: 2706.2368\n",
      "Epoch 125/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2441.0698 - val_loss: 2773.8643\n",
      "Epoch 126/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2428.3865 - val_loss: 2583.6040\n",
      "Epoch 127/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2363.2708 - val_loss: 2597.6841\n",
      "Epoch 128/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2377.5320 - val_loss: 2595.2878\n",
      "Epoch 129/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2373.2175 - val_loss: 2588.9255\n",
      "Epoch 130/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2356.4277 - val_loss: 2608.9365\n",
      "Epoch 131/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2386.3398 - val_loss: 2679.3850\n",
      "Epoch 132/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2341.9788 - val_loss: 2579.7039\n",
      "Epoch 133/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2335.4490 - val_loss: 2534.3342\n",
      "Epoch 134/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2365.2615 - val_loss: 2595.8105\n",
      "Epoch 135/200\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 2483.7825 - val_loss: 2611.1191\n",
      "Epoch 136/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2313.1802 - val_loss: 2552.0547\n",
      "Epoch 137/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2295.6333 - val_loss: 2534.9302\n",
      "Epoch 138/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2291.8823 - val_loss: 2592.6282\n",
      "Epoch 139/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2307.2510 - val_loss: 2599.8542\n",
      "Epoch 140/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2331.5461 - val_loss: 2566.3113\n",
      "Epoch 141/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2299.5803 - val_loss: 2523.5093\n",
      "Epoch 142/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2373.8474 - val_loss: 2537.3247\n",
      "Epoch 143/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2347.1292 - val_loss: 2703.6587\n",
      "Epoch 144/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2242.1135 - val_loss: 2539.5217\n",
      "Epoch 145/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2259.7588 - val_loss: 2511.4924\n",
      "Epoch 146/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2236.8687 - val_loss: 2514.7678\n",
      "Epoch 147/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2285.6589 - val_loss: 2537.2251\n",
      "Epoch 148/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2315.6812 - val_loss: 2497.0974\n",
      "Epoch 149/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2262.9771 - val_loss: 2642.5691\n",
      "Epoch 150/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2381.1584 - val_loss: 2580.0378\n",
      "Epoch 151/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2396.2258 - val_loss: 2654.6074\n",
      "Epoch 152/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2437.7634 - val_loss: 2699.9895\n",
      "Epoch 153/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2716.9602 - val_loss: 2892.1934\n",
      "Epoch 154/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2465.7378 - val_loss: 2527.6497\n",
      "Epoch 155/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2292.8765 - val_loss: 2498.2520\n",
      "Epoch 156/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2252.3765 - val_loss: 2462.8403\n",
      "Epoch 157/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2223.2073 - val_loss: 2489.7947\n",
      "Epoch 158/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2280.5605 - val_loss: 2493.1938\n",
      "Epoch 159/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2268.4775 - val_loss: 2643.7544\n",
      "Epoch 160/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2226.3191 - val_loss: 2614.0640\n",
      "Epoch 161/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2628.5959 - val_loss: 2695.4028\n",
      "Epoch 162/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2278.4910 - val_loss: 2747.1333\n",
      "Epoch 163/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2306.8257 - val_loss: 2574.6782\n",
      "Epoch 164/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2185.8906 - val_loss: 2413.4829\n",
      "Epoch 165/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2190.5200 - val_loss: 2451.7913\n",
      "Epoch 166/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2289.4885 - val_loss: 2682.0322\n",
      "Epoch 167/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2255.1262 - val_loss: 2547.9077\n",
      "Epoch 168/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2174.5457 - val_loss: 2553.4700\n",
      "Epoch 169/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2159.7209 - val_loss: 2483.1143\n",
      "Epoch 170/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2284.0461 - val_loss: 2433.1941\n",
      "Epoch 171/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2233.7527 - val_loss: 2639.3291\n",
      "Epoch 172/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2196.7712 - val_loss: 2532.0903\n",
      "Epoch 173/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2174.2314 - val_loss: 2505.1924\n",
      "Epoch 174/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2172.5571 - val_loss: 2572.7043\n",
      "Epoch 175/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2312.1038 - val_loss: 2551.4136\n",
      "Epoch 176/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2187.6760 - val_loss: 2533.6980\n",
      "Epoch 177/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2178.2544 - val_loss: 2639.0972\n",
      "Epoch 178/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2135.7544 - val_loss: 2491.2512\n",
      "Epoch 179/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2182.1843 - val_loss: 2442.3677\n",
      "Epoch 180/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2193.0496 - val_loss: 2408.6787\n",
      "Epoch 181/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2116.2532 - val_loss: 2488.3333\n",
      "Epoch 182/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2168.1072 - val_loss: 2531.6594\n",
      "Epoch 183/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2168.1892 - val_loss: 2554.6218\n",
      "Epoch 184/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2226.6777 - val_loss: 2441.5679\n",
      "Epoch 185/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2136.6482 - val_loss: 2446.2124\n",
      "Epoch 186/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2097.0735 - val_loss: 2621.3755\n",
      "Epoch 187/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2156.7002 - val_loss: 2591.8430\n",
      "Epoch 188/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2357.9861 - val_loss: 2378.9717\n",
      "Epoch 189/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2205.8469 - val_loss: 2398.1543\n",
      "Epoch 190/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2080.7229 - val_loss: 2507.9563\n",
      "Epoch 191/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2085.8523 - val_loss: 2585.4954\n",
      "Epoch 192/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2129.1870 - val_loss: 2487.9377\n",
      "Epoch 193/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2091.6062 - val_loss: 2527.6626\n",
      "Epoch 194/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2146.6040 - val_loss: 2624.1265\n",
      "Epoch 195/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2143.5601 - val_loss: 2606.3738\n",
      "Epoch 196/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2117.0576 - val_loss: 2445.2444\n",
      "Epoch 197/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2116.4326 - val_loss: 2595.6853\n",
      "Epoch 198/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2103.8660 - val_loss: 2539.8521\n",
      "Epoch 199/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2080.7261 - val_loss: 2722.8547\n",
      "Epoch 200/200\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 2085.5789 - val_loss: 2420.6692\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 1981.1959\n",
      "Evaluation results: 1981.1959228515625\n"
     ]
    }
   ],
   "source": [
    "total_x = total_x.astype(np.float32)\n",
    "total_y = total_y.astype(np.float32)\n",
    "total_x[np.isinf(total_x)] = np.nan\n",
    "total_x[np.abs(total_x) > 1e6] = np.nan\n",
    "# Handle NaN values by replacing them with zeros (you can choose a different strategy)\n",
    "total_x[np.isnan(total_x)] = 0.0\n",
    "scaler = MinMaxScaler()\n",
    "total_x_normalized = scaler.fit_transform(total_x.reshape(-1, total_x.shape[-1])).reshape(total_x.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_x_normalized, total_y, test_size=0.2, random_state=42)\n",
    "model = train_module(X_train, y_train, X_test, y_test, epoch=300, learning_rate=0.001, batch=32)\n",
    "evaluation_results = model.evaluate(X_train, y_train)\n",
    "print(\"Evaluation results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 5ms/step\n",
      "AUROC: 0.844043201696263\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred_proba = np.where(y_pred_proba > 0, 1, 0)\n",
    "y_label = np.where(y_test > 0, 1, 0)\n",
    "#print(y_pred_proba)\n",
    "# Calculate AUROC\n",
    "auroc = roc_auc_score(y_label, y_pred_proba)\n",
    "print(\"AUROC:\", auroc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
