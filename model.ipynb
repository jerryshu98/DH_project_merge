{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or '3' to suppress all messages\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import random\n",
    "import joblib\n",
    "label_encoder = LabelEncoder()\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'charttime', 'before_weaning_hr', 'stay_id', 'O2_flow',\n",
      "       'heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'spo2', 'peep', 'fio2',\n",
      "       'tidal_volume_observed', 'respiratory_rate_set', 'plateau_pressure',\n",
      "       'ventilator_mode', 'GCS', 'age_now', 'gender', 'insurance', 'race',\n",
      "       'admission_type', 'first_careunit', 'tobacco', 'label', 'Rev_h',\n",
      "       'dod_h', 'RSBI', 'minute_ventilation', 'ventilator_mode_group', 'BMI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "label_path = './data/data_by_table/pre_24h_data_v6.csv'\n",
    "flag_data_path = './data/data_by_table/ground_truth.csv'\n",
    "raw_data_path = './data/data_by_table/pre_24h_data_1217.csv'\n",
    "mode_data_path = './data/data_by_table/pre_24_merged_30_rows_12_07.csv'\n",
    "\n",
    "data_df = pd.read_csv(raw_data_path)\n",
    "flag_data_df = pd.read_csv(flag_data_path)\n",
    "label_df = pd.read_csv(label_path)\n",
    "mode_df = pd.read_csv(mode_data_path)\n",
    "data_df['BMI'] = data_df['weight_kg'] / ((data_df['height_cm'] / 100) ** 2)\n",
    "data_df['gender'] = label_encoder.fit_transform(data_df['gender'])\n",
    "data_df['race'] = label_encoder.fit_transform(data_df['race'])\n",
    "data_df['first_careunit'] = label_encoder.fit_transform(data_df['first_careunit'])\n",
    "data_df['admission_type'] = label_encoder.fit_transform(data_df['admission_type'])\n",
    "data_df['ventilator_mode_group'] = label_encoder.fit_transform(data_df['ventilator_mode_group'])\n",
    "data_df['ventilator_mode'] = label_encoder.fit_transform(data_df['ventilator_mode'])\n",
    "data_df['insurance'] = label_encoder.fit_transform(data_df['insurance'])\n",
    "data_df = data_df.drop(columns=['height_cm', 'weight_kg'])\n",
    "data_df['RSBI'] =   data_df['resp_rate']/(data_df['tidal_volume_observed']* 0.001) \n",
    "data_df['minute_ventilation'] = data_df['tidal_volume_observed'] * data_df['resp_rate']* 0.001\n",
    "data_df = data_df.drop(columns=[ 'hadm_id','subject_id'])\n",
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitalsign = ['heart_rate', 'sbp', 'dbp', 'mbp', 'spo2', 'resp_rate', \n",
    "             'tidal_volume_observed', 'RSBI', 'minute_ventilation']\n",
    "ventilator_settings = ['peep', 'fio2', 'respiratory_rate_set', 'plateau_pressure']\n",
    "baseline = ['age_now','gender', 'insurance', 'race', 'admission_type', 'first_careunit'\n",
    ",'weight_kg', 'height_cm', 'tobacco' ]\n",
    "all_feature = ['heart_rate', 'sbp', 'dbp', 'mbp', 'spo2', 'resp_rate', \n",
    "             'tidal_volume_observed', 'RSBI', 'minute_ventilation','peep',\n",
    "              'fio2', 'respiratory_rate_set', 'plateau_pressure','age_now','gender', 'insurance',\n",
    "               'race', 'admission_type', 'first_careunit'\n",
    ",'weight_kg', 'height_cm', 'tobacco']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BMI',\n",
       " 'GCS',\n",
       " 'O2_flow',\n",
       " 'Unnamed: 0',\n",
       " 'before_weaning_hr',\n",
       " 'ventilator_mode',\n",
       " 'ventilator_mode_group'}"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['Unnamed: 0', 'before_weaning_hr', 'O2_flow', 'heart_rate', 'sbp',\n",
    "       'dbp', 'mbp', 'resp_rate', 'spo2', 'peep', 'fio2',\n",
    "       'tidal_volume_observed', 'respiratory_rate_set', 'plateau_pressure',\n",
    "       'ventilator_mode', 'GCS', 'age_now', 'gender', 'insurance', 'race',\n",
    "       'admission_type', 'first_careunit', 'tobacco', 'RSBI',\n",
    "       'minute_ventilation', 'ventilator_mode_group', 'BMI']) - set(all_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "68155    1\n",
      "68156    1\n",
      "68157    1\n",
      "68158    1\n",
      "68159    1\n",
      "Name: ventilator_mode_group, Length: 68160, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_df['ventilator_mode_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create reasonable data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    \n",
    "    if df.isna().any().any():\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "def get_label(id_df):\n",
    "    label = 0\n",
    "    if not check_missing_values(id_df):\n",
    "            label = id_df['label'].iloc[0]\n",
    "            if label == 1:\n",
    "                label = 48\n",
    "            else:\n",
    "                if id_df['Rev_h'].iloc[0] != -1000:\n",
    "                    label = -(48 - id_df['Rev_h'].iloc[0])\n",
    "                    #label = -48\n",
    "                elif id_df['dod_h'].iloc[0] != -1000 and id_df['dod_h'].iloc[0]>0 and id_df['dod_h'].iloc[0]<48:\n",
    "                    label = -(96 - id_df['dod_h'].iloc[0]*2)\n",
    "                    #label = -96\n",
    "                else:\n",
    "                    label = -96\n",
    "                     \n",
    "    return label\n",
    "    \n",
    "def create_patient_group(label_df,mode_df,kick = 0):\n",
    "    alive_list = [[],[],[],[],[],[]]\n",
    "    dead_list = [[],[],[],[],[],[]]\n",
    "    alive_num = [0,0,0,0,0]\n",
    "    dead_num = [0,0,0,0,0]\n",
    "    patient_set = set(mode_df['stay_id'])\n",
    "    for index, row in flag_data_df.iterrows():\n",
    "        if row['stay_id'] not in patient_set:\n",
    "            continue\n",
    "        id_mode_df = mode_df[mode_df['stay_id'] == row['stay_id']]\n",
    "        count_complete_mode = id_mode_df['ventilator_mode_group'].tail(12).value_counts().get('Complete Support', 0)\n",
    "        group_num = int(math.floor(count_complete_mode/3))\n",
    "        label = id_mode_df['label'].iloc[0]\n",
    "        if get_label(label_df[label_df['stay_id'] == row['stay_id']]) == -96 and kick:\n",
    "            continue\n",
    "        if label == 1:\n",
    "            alive_list[group_num].append(row['stay_id'])\n",
    "            alive_num[group_num]+=1\n",
    "        else:\n",
    "            dead_list[group_num].append(row['stay_id'])\n",
    "            dead_num[group_num]+=1\n",
    "    return alive_list, dead_list, alive_num, dead_num\n",
    "    \n",
    "def split_list(input_list):\n",
    "    random.shuffle(input_list)\n",
    "    total_length = len(input_list)\n",
    "    part1_length = int(total_length * 0.7)\n",
    "    part2_length = int(total_length * 0.2)\n",
    "    part3_length = total_length - part1_length - part2_length\n",
    "    part1 = random.sample(input_list, part1_length)\n",
    "    remaining_list = [element for element in input_list if element not in part1]\n",
    "\n",
    "    part2 = random.sample(remaining_list, part2_length)\n",
    "    part3 = [element for element in remaining_list if element not in part2]\n",
    "    #print(total_length, len(part1),len(part2),len(part3))\n",
    "    return part1, part2, part3\n",
    "\n",
    "def create_data(alive_list, dead_list):\n",
    "    train_data_id = []\n",
    "    val_data_id = []\n",
    "    test_data_id = []\n",
    "    train_data_id_2 = []\n",
    "    val_data_id_2 = []\n",
    "    test_data_id_2 = []\n",
    "    for i in range(5):\n",
    "        train, val, test = split_list(alive_list[i])\n",
    "        train_data_id+=train\n",
    "        val_data_id+=(val)\n",
    "        test_data_id+=(test)\n",
    "        train, val, test = split_list(dead_list[i])\n",
    "        train_data_id_2+=train\n",
    "        val_data_id_2+=(val)\n",
    "        test_data_id_2+=(test)\n",
    "    \n",
    "    #print(len(train_data_id)/(len(train_data_id)+len(train_data_id_2)))\n",
    "    #print(len(val_data_id)/(len(val_data_id)+len(val_data_id_2)))\n",
    "    #print(len(test_data_id)/(len(test_data_id)+len(test_data_id_2)))\n",
    "    return train_data_id+train_data_id_2, val_data_id+val_data_id_2, test_data_id+test_data_id_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971\n",
      "1860 529 274\n"
     ]
    }
   ],
   "source": [
    "alive_list, dead_list, alive_num, dead_num = create_patient_group(label_df,mode_df)\n",
    "train_data_id, val_data_id, test_data_id = create_data(alive_list, dead_list)\n",
    "num = 0\n",
    "for i in range (5):\n",
    "    num += len(dead_list[i])\n",
    "print(num)\n",
    "print(len(train_data_id), len(val_data_id),len(test_data_id)) #1603 455 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列表1和列表2沒有相同的元素\n",
      "列表1和列表3沒有相同的元素\n",
      "列表2和列表3沒有相同的元素\n"
     ]
    }
   ],
   "source": [
    "def check_duplicate_elements(list1, list2, list3):\n",
    "    common_elements_12 = set(list1) & set(list2)\n",
    "    common_elements_13 = set(list1) & set(list3)\n",
    "    common_elements_23 = set(list2) & set(list3)\n",
    "\n",
    "    if common_elements_12:\n",
    "        print(f\"列表1和列表2有相同的元素：{common_elements_12}\")\n",
    "    else:\n",
    "        print(\"列表1和列表2沒有相同的元素\")\n",
    "\n",
    "    if common_elements_13:\n",
    "        print(f\"列表1和列表3有相同的元素：{common_elements_13}\")\n",
    "    else:\n",
    "        print(\"列表1和列表3沒有相同的元素\")\n",
    "\n",
    "    if common_elements_23:\n",
    "        print(f\"列表2和列表3有相同的元素：{common_elements_23}\")\n",
    "    else:\n",
    "        print(\"列表2和列表3沒有相同的元素\")\n",
    "\n",
    "check_duplicate_elements(train_data_id, val_data_id, test_data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = 'group_data/77_77_77/train_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(train_data_id):\n",
    "        csv_writer.writerow(row)\n",
    "csv_file_name = 'group_data/77_77_77/val_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(val_data_id):\n",
    "        csv_writer.writerow(row)\n",
    "csv_file_name = 'group_data/77_77_77/test_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(test_data_id):\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_csv(file_name):\n",
    "    data = []\n",
    "    with open(file_name, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            # 將每個元素轉換為數字\n",
    "            row = [int(element) for element in row]\n",
    "            data.append(row[0])\n",
    "    return data\n",
    "\n",
    "group_prefix = 'group_data/1216best/'\n",
    "\n",
    "# 讀取訓練數據\n",
    "train_csv_file = group_prefix + 'train_data_id.csv'\n",
    "train_data_id = read_from_csv(train_csv_file)\n",
    "\n",
    "# 讀取驗證數據\n",
    "val_csv_file = group_prefix + 'val_data_id.csv'\n",
    "val_data_id = read_from_csv(val_csv_file)\n",
    "\n",
    "# 讀取測試數據\n",
    "test_csv_file = group_prefix + 'test_data_id.csv'\n",
    "test_data_id = read_from_csv(test_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_value(df, colname, start, end):\n",
    "    start = 23 - start\n",
    "    end = 23 - end\n",
    "    df = df.reset_index()\n",
    "    return (df[colname].iloc[end] + df[colname].iloc[end-1] + df[colname].iloc[end-2]) - (df[colname].iloc[start] + df[colname].iloc[start+1] + df[colname].iloc[start+2])\n",
    "\n",
    "def get_more_feature(df, colnames, start, end):\n",
    "    add_list = []\n",
    "    for name in colnames:\n",
    "        now = get_diff_value(df, name, start, end)\n",
    "        add_list.append(now)\n",
    "    return np.array(add_list)\n",
    "\n",
    "def false_percentage(y_label):\n",
    "    zero = len(y_label) - np.count_nonzero(y_label)\n",
    "    print(f\"false percentage: {(zero/len(y_label)) * 100:.2f}%\")\n",
    "\n",
    "def calculate_tpr_tnr(y_true, y_pred):\n",
    "    # 计算 TP, FN, TN, FP\n",
    "    TP = sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    # 计算 TPR 和 TNR\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    return TPR, TNR\n",
    "\n",
    "def calculate_tpr_tnr2(y_true, y_pred):\n",
    "    # 计算 TP, FN, TN, FP\n",
    "    TP = sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    # 计算 TPR 和 TNR\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    return TPR, TNR\n",
    "\n",
    "def get_label(id_df):\n",
    "    label = 0\n",
    "    if not check_missing_values(id_df):\n",
    "            label = id_df['label'].iloc[0]\n",
    "            if label == 1:\n",
    "                label = 48\n",
    "            else:\n",
    "                if id_df['Rev_h'].iloc[0] != -1000:\n",
    "                    label = -(48 - id_df['Rev_h'].iloc[0])\n",
    "                elif id_df['dod_h'].iloc[0] != -1000 and id_df['dod_h'].iloc[0]>0 and id_df['dod_h'].iloc[0]<48:\n",
    "                    label = -(96 - id_df['dod_h'].iloc[0]*2)\n",
    "                else:\n",
    "                    label = -96 \n",
    "    return label\n",
    "\n",
    "def get_data(flag_data_df, data_df,label_df, mode_df,train_data_id, val_data_id, test_data_id,hour = 23):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    count = 0\n",
    "    cc = 0\n",
    "    aug_data = ['heart_rate', \n",
    "       'resp_rate', 'spo2', 'peep', 'fio2', 'tidal_volume_observed',\n",
    "       'respiratory_rate_set', 'plateau_pressure']\n",
    "    for index, row in flag_data_df.iterrows():\n",
    "        id_df = data_df[data_df['stay_id'] == row['stay_id']]\n",
    "        id_mode_df = mode_df[mode_df['stay_id'] == row['stay_id']]['ventilator_mode_group']\n",
    "        id_df_label = label_df[label_df['stay_id'] == row['stay_id']]\n",
    "        if not check_missing_values(id_df):\n",
    "            label = get_label(id_df_label)\n",
    "            id_now = id_df['stay_id'].iloc[0]\n",
    "            id_df = id_df.drop(columns='stay_id')\n",
    "            id_df = id_df.drop(columns='label')\n",
    "            id_df = id_df.drop(columns='charttime')\n",
    "            id_df = id_df.drop(columns='Rev_h')\n",
    "            id_df = id_df.drop(columns='dod_h')\n",
    "            print(id_df.columns)\n",
    "            mode_code = 0\n",
    "            count_complete_mode =  mode_df[mode_df['stay_id'] == row['stay_id']]['ventilator_mode_group'].tail(12).value_counts().get('Complete Support', 0)\n",
    "            if(id_df.shape[0] != 24):\n",
    "                continue\n",
    "            zero_hr_values = id_df.iloc[hour, :].values\n",
    "            try:\n",
    "                count+=1\n",
    "                if(id_mode_df.iloc[-1] == 'Complete Support'):\n",
    "                    mode_code = 1\n",
    "                    cc+=1\n",
    "            except:\n",
    "                continue\n",
    "            #zero_hr_values = np.append(zero_hr_values, mode_code)\n",
    "            zero_hr_values = np.append(zero_hr_values, count_complete_mode)\n",
    "            #print( get_more_feature(id_df,aug_data,0,12).shape)\n",
    "            #zero_hr_values = np.append(zero_hr_values, get_more_feature(id_df,aug_data,12,0))\n",
    "            #zero_hr_values = generate_more_feature(id_df, aug_columns ,zero_hr_values)\n",
    "            if id_now in train_data_id:\n",
    "                train_x.append(zero_hr_values)\n",
    "                train_y.append(label)\n",
    "            elif id_now in val_data_id:\n",
    "                val_x.append(zero_hr_values)\n",
    "                val_y.append(label)\n",
    "            elif id_now in test_data_id:\n",
    "                test_x.append(zero_hr_values)\n",
    "                test_y.append(label)\n",
    "    #total_x = np.array(total_x)\n",
    "    #total_y = np.array(total_y).reshape(-1, 1)\n",
    "    #print(cc/count*100)\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_value(df, colname, start, end):\n",
    "    start = 23 - start\n",
    "    end = 23 - end\n",
    "    df = df.reset_index()\n",
    "    return (df[colname].iloc[end] + df[colname].iloc[end-1] + df[colname].iloc[end-2]) - (df[colname].iloc[start] + df[colname].iloc[start+1] + df[colname].iloc[start+2])\n",
    "\n",
    "def get_more_feature(df, colnames, start, end):\n",
    "    add_list = []\n",
    "    for name in colnames:\n",
    "        now = get_diff_value(df, name, start, end)\n",
    "        add_list.append(now)\n",
    "    return np.array(add_list)\n",
    "\n",
    "def false_percentage(y_label):\n",
    "    zero = len(y_label) - np.count_nonzero(y_label)\n",
    "    print(f\"false percentage: {(zero/len(y_label)) * 100:.2f}%\")\n",
    "\n",
    "def calculate_tpr_tnr(y_true, y_pred):\n",
    "    # 计算 TP, FN, TN, FP\n",
    "    TP = sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    # 计算 TPR 和 TNR\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    return TPR, TNR\n",
    "\n",
    "def calculate_tpr_tnr2(y_true, y_pred):\n",
    "    # 计算 TP, FN, TN, FP\n",
    "    TP = sum((y_true == 1) & (y_pred == 1))\n",
    "    FN = sum((y_true == 1) & (y_pred == 0))\n",
    "    TN = sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    # 计算 TPR 和 TNR\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    return TPR, TNR\n",
    "\n",
    "def get_label(id_df):\n",
    "    label = 0\n",
    "    if not check_missing_values(id_df):\n",
    "            label = id_df['label'].iloc[0]\n",
    "            if label == 1:\n",
    "                label = 48\n",
    "            else:\n",
    "                if id_df['Rev_h'].iloc[0] != -1000:\n",
    "                    label = -(48 - id_df['Rev_h'].iloc[0])\n",
    "                elif id_df['dod_h'].iloc[0] != -1000 and id_df['dod_h'].iloc[0]>0 and id_df['dod_h'].iloc[0]<48:\n",
    "                    label = -(96 - id_df['dod_h'].iloc[0]*2)\n",
    "                else:\n",
    "                    label = -96 \n",
    "    return label\n",
    "\n",
    "def get_data(flag_data_df, data_df,label_df, mode_df,train_data_id, val_data_id, test_data_id,hour = 23):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    count = 0\n",
    "    cc = 0\n",
    "    all_feature = ['heart_rate', 'sbp', 'dbp', 'mbp', 'spo2', 'resp_rate', \n",
    "             'tidal_volume_observed', 'RSBI', 'minute_ventilation','peep',\n",
    "              'fio2', 'respiratory_rate_set', 'plateau_pressure','age_now','gender', 'insurance',\n",
    "               'race', 'admission_type', 'first_careunit'\n",
    "                ,'BMI', 'tobacco','BMI','GCS']\n",
    "    aug_data = ['heart_rate', \n",
    "       'resp_rate', 'spo2', 'peep', 'fio2', 'tidal_volume_observed',\n",
    "       'respiratory_rate_set', 'plateau_pressure']\n",
    "    for index, row in flag_data_df.iterrows():\n",
    "        id_df = data_df[data_df['stay_id'] == row['stay_id']]\n",
    "        id_mode_df = mode_df[mode_df['stay_id'] == row['stay_id']]['ventilator_mode_group']\n",
    "        id_df_label = label_df[label_df['stay_id'] == row['stay_id']]\n",
    "        if not check_missing_values(id_df):\n",
    "            label = get_label(id_df_label)\n",
    "            id_now = id_df['stay_id'].iloc[0]\n",
    "            id_df = id_df[all_feature]\n",
    "            if(id_df.shape[0] != 24):\n",
    "                continue\n",
    "            \n",
    "            zero_hr_values = id_df.iloc[hour, :].values\n",
    "            count_complete_mode =  mode_df[mode_df['stay_id'] == row['stay_id']]['ventilator_mode_group'].tail(12).value_counts().get('Complete Support', 0)\n",
    "            if(id_df.shape[0] != 24):\n",
    "                continue\n",
    "            zero_hr_values = id_df.iloc[hour, :].values\n",
    "            try:\n",
    "                count+=1\n",
    "                if(id_mode_df.iloc[-1] == 'Complete Support'):\n",
    "                    mode_code = 1\n",
    "                    cc+=1\n",
    "            except:\n",
    "                continue\n",
    "            #zero_hr_values = np.append(zero_hr_values, mode_code)\n",
    "            #zero_hr_values = np.append(zero_hr_values, count_complete_mode)\n",
    "            if id_now in train_data_id:\n",
    "                train_x.append(zero_hr_values)\n",
    "                train_y.append(label)\n",
    "            elif id_now in val_data_id:\n",
    "                val_x.append(zero_hr_values)\n",
    "                val_y.append(label)\n",
    "            elif id_now in test_data_id:\n",
    "                test_x.append(zero_hr_values)\n",
    "                test_y.append(label)\n",
    "    #total_x = np.array(total_x)\n",
    "    #total_y = np.array(total_y).reshape(-1, 1)\n",
    "    #print(cc/count*100)\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852 527 274\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = get_data(flag_data_df, data_df,label_df, mode_df,train_data_id, val_data_id, test_data_id)\n",
    "print(len(train_x), len(val_x), len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percentage_of_negative_values(arr):\n",
    "    # 檢查輸入是否為 NumPy 陣列\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        print(\"請輸入有效的 NumPy 陣列。\")\n",
    "        return\n",
    "    \n",
    "    # 計算小於 0 的值的百分比\n",
    "    negative_percentage = np.count_nonzero(arr < 0) / arr.size * 100\n",
    "    \n",
    "    # 印出結果\n",
    "    print(f\"陣列中 {negative_percentage:.2f}% 的值小於 0。\")\n",
    "    \n",
    "def normalize_data(train_x, train_y, val_x, val_y, test_x, test_y):\n",
    "    total_x = train_x+val_x+test_x\n",
    "    total_x = np.array(total_x)\n",
    "    total_x = total_x.astype(np.float32)\n",
    "    total_x[np.isinf(total_x)] = np.nan\n",
    "    total_x[np.abs(total_x) > 1e6] = np.nan\n",
    "    total_x[np.isnan(total_x)] = 0.0\n",
    "    scaler = MinMaxScaler()\n",
    "    total_x_normalized = scaler.fit_transform(total_x.reshape(-1, total_x.shape[-1])).reshape(total_x.shape)\n",
    "\n",
    "    X_train = total_x_normalized[:len(train_x)]\n",
    "    X_val = total_x_normalized[len(train_x):len(train_x)+len(val_x)]\n",
    "    X_test = total_x_normalized[len(train_x)+len(val_x):len(train_x)+len(val_x)+len(test_x)]\n",
    "    y_train = np.array(train_y).reshape(-1, 1)\n",
    "    y_val = np.array(val_y).reshape(-1, 1)\n",
    "    y_test = np.array(test_y).reshape(-1, 1)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1852, 23)\n",
      "陣列中 36.56% 的值小於 0。\n",
      "(527, 23)\n",
      "陣列中 36.62% 的值小於 0。\n",
      "(274, 23)\n",
      "陣列中 36.50% 的值小於 0。\n"
     ]
    }
   ],
   "source": [
    "total_x = train_x+val_x+test_x\n",
    "total_x = np.array(total_x)\n",
    "total_x = total_x.astype(np.float32)\n",
    "total_x[np.isinf(total_x)] = np.nan\n",
    "total_x[np.abs(total_x) > 1e6] = np.nan\n",
    "total_x[np.isnan(total_x)] = 0.0\n",
    "scaler = MinMaxScaler()\n",
    "scaler = joblib.load('./model/C_scaler.joblib')\n",
    "total_x_normalized = scaler.transform(total_x.reshape(-1, total_x.shape[-1])).reshape(total_x.shape)\n",
    "joblib.dump(scaler, './model/C_scaler.joblib')\n",
    "\n",
    "X_train = total_x_normalized[:len(train_x)]\n",
    "X_val = total_x_normalized[len(train_x):len(train_x)+len(val_x)]\n",
    "X_test = total_x_normalized[len(train_x)+len(val_x):len(train_x)+len(val_x)+len(test_x)]\n",
    "y_train = np.array(train_y).reshape(-1, 1)\n",
    "y_val = np.array(val_y).reshape(-1, 1)\n",
    "y_test = np.array(test_y).reshape(-1, 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print_percentage_of_negative_values(y_train)\n",
    "print(X_val.shape)\n",
    "print_percentage_of_negative_values(y_val)\n",
    "print(X_test.shape)\n",
    "print_percentage_of_negative_values(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_NN_module(X_train, y_train, X_test, y_test, epoch, learning_rate, batch, verbose=1):\n",
    "    # Define model and train\n",
    "    def build_nn_model(input_shape):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_nn_model(input_shape=(X_train.shape[1],))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epoch, batch_size=batch, validation_data=(X_test, y_test), verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, prefix):\n",
    "    print(f\"{prefix} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "def print_score(model, X_data, y_data, who):\n",
    "    print(\"=========\"+who+\"=========\")\n",
    "    y_pred_proba = model.predict(X_data, verbose=0)\n",
    "    y_pred = np.where(y_pred_proba >0 , 1, 0)\n",
    "    y_label = np.where(y_data > 0, 1, 0)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_label)\n",
    "    tpr, tnr = calculate_tpr_tnr(y_label, y_pred)\n",
    "    print(\"TPR:\", tpr)\n",
    "    print(\"TNR:\", tnr)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    auroc = roc_auc_score(y_label, y_pred_proba)\n",
    "    print(\"AUROC:\", auroc)\n",
    "    print_classification_metrics(y_label,y_pred, who)\n",
    "def get_score(model, X_data, y_data, who):\n",
    "    y_pred_proba = model.predict(X_data, verbose=0)\n",
    "    y_pred = np.where(y_pred_proba >0 , 1, 0)\n",
    "    y_label = np.where(y_data > 0, 1, 0)\n",
    "\n",
    "    accuracy = np.mean(y_pred == y_label)\n",
    "    tpr, tnr = calculate_tpr_tnr(y_label, y_pred)\n",
    "    auroc = roc_auc_score(y_label, y_pred_proba)\n",
    "    return auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "58/58 [==============================] - 8s 102ms/step - loss: 3061.6665 - val_loss: 3130.2141\n",
      "Epoch 2/45\n",
      "58/58 [==============================] - 1s 13ms/step - loss: 2881.4880 - val_loss: 2819.7043\n",
      "Epoch 3/45\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2522.7251 - val_loss: 2343.5671\n",
      "Epoch 4/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2245.3071 - val_loss: 2135.5325\n",
      "Epoch 5/45\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 2187.7642 - val_loss: 2097.3003\n",
      "Epoch 6/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2152.8872 - val_loss: 2069.3945\n",
      "Epoch 7/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2152.0720 - val_loss: 2105.3823\n",
      "Epoch 8/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2129.8210 - val_loss: 2059.2458\n",
      "Epoch 9/45\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2142.7336 - val_loss: 2064.7104\n",
      "Epoch 10/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2114.6641 - val_loss: 2052.0068\n",
      "Epoch 11/45\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2118.9719 - val_loss: 2050.0376\n",
      "Epoch 12/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2108.4651 - val_loss: 2084.8186\n",
      "Epoch 13/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2117.1643 - val_loss: 2096.3586\n",
      "Epoch 14/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2110.9690 - val_loss: 2076.0996\n",
      "Epoch 15/45\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 2111.0457 - val_loss: 2063.0054\n",
      "Epoch 16/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2106.9421 - val_loss: 2074.9724\n",
      "Epoch 17/45\n",
      "58/58 [==============================] - 0s 4ms/step - loss: 2099.4944 - val_loss: 2039.2445\n",
      "Epoch 18/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2088.8298 - val_loss: 2049.1653\n",
      "Epoch 19/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2089.4219 - val_loss: 2036.1655\n",
      "Epoch 20/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2083.7209 - val_loss: 2031.5551\n",
      "Epoch 21/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2082.2969 - val_loss: 2047.2568\n",
      "Epoch 22/45\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 2090.5791 - val_loss: 2068.1777\n",
      "Epoch 23/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2084.2759 - val_loss: 2037.1281\n",
      "Epoch 24/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2080.6035 - val_loss: 2045.9806\n",
      "Epoch 25/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2080.0129 - val_loss: 2038.0059\n",
      "Epoch 26/45\n",
      "58/58 [==============================] - 1s 18ms/step - loss: 2087.5144 - val_loss: 2070.2871\n",
      "Epoch 27/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2076.9182 - val_loss: 2044.7875\n",
      "Epoch 28/45\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 2068.5615 - val_loss: 2062.8481\n",
      "Epoch 29/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2069.1262 - val_loss: 2037.4767\n",
      "Epoch 30/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2060.5918 - val_loss: 2039.3909\n",
      "Epoch 31/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2069.4414 - val_loss: 2039.5237\n",
      "Epoch 32/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2076.2285 - val_loss: 2035.3074\n",
      "Epoch 33/45\n",
      "58/58 [==============================] - 0s 6ms/step - loss: 2062.1118 - val_loss: 2061.8296\n",
      "Epoch 34/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2068.3020 - val_loss: 2109.1133\n",
      "Epoch 35/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2064.4265 - val_loss: 2043.2258\n",
      "Epoch 36/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2068.7822 - val_loss: 2065.6604\n",
      "Epoch 37/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2064.3325 - val_loss: 2060.6592\n",
      "Epoch 38/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2052.3784 - val_loss: 2039.8599\n",
      "Epoch 39/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2056.3735 - val_loss: 2042.2144\n",
      "Epoch 40/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2047.5696 - val_loss: 2032.8280\n",
      "Epoch 41/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2043.1082 - val_loss: 2052.3547\n",
      "Epoch 42/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2041.1273 - val_loss: 2032.6426\n",
      "Epoch 43/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2035.4230 - val_loss: 2043.9724\n",
      "Epoch 44/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2034.7334 - val_loss: 2041.9980\n",
      "Epoch 45/45\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 2025.6729 - val_loss: 2034.0353\n"
     ]
    }
   ],
   "source": [
    "model = train_NN_module(X_train, y_train, X_val, y_val, epoch=45, learning_rate=0.001, batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========train=========\n",
      "TPR: [0.89787234]\n",
      "TNR: [0.44460857]\n",
      "Accuracy: 73.22%\n",
      "AUROC: 0.749446557088532\n",
      "train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.44      0.55       677\n",
      "           1       0.74      0.90      0.81      1175\n",
      "\n",
      "    accuracy                           0.73      1852\n",
      "   macro avg       0.73      0.67      0.68      1852\n",
      "weighted avg       0.73      0.73      0.71      1852\n",
      "\n",
      "\n",
      "=========val=========\n",
      "TPR: [0.89520958]\n",
      "TNR: [0.43523316]\n",
      "Accuracy: 72.68%\n",
      "AUROC: 0.7682665756569762\n",
      "val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.44      0.54       193\n",
      "           1       0.73      0.90      0.81       334\n",
      "\n",
      "    accuracy                           0.73       527\n",
      "   macro avg       0.72      0.67      0.67       527\n",
      "weighted avg       0.72      0.73      0.71       527\n",
      "\n",
      "\n",
      "=========test=========\n",
      "TPR: [0.94252874]\n",
      "TNR: [0.52]\n",
      "Accuracy: 78.83%\n",
      "AUROC: 0.8039080459770115\n",
      "test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.52      0.64       100\n",
      "           1       0.77      0.94      0.85       174\n",
      "\n",
      "    accuracy                           0.79       274\n",
      "   macro avg       0.81      0.73      0.75       274\n",
      "weighted avg       0.80      0.79      0.77       274\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_score(model, X_train,y_train,'train')\n",
    "print_score(model, X_val,y_val,'val')\n",
    "print_score(model, X_test,y_test,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========test=========\n",
      "TPR: [0.93678161]\n",
      "TNR: [0.56]\n",
      "Accuracy: 79.93%\n",
      "AUROC: 0.8077011494252874\n",
      "test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.56      0.67       100\n",
      "           1       0.79      0.94      0.86       174\n",
      "\n",
      "    accuracy                           0.80       274\n",
      "   macro avg       0.81      0.75      0.76       274\n",
      "weighted avg       0.81      0.80      0.79       274\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('./model/1219test.h5')\n",
    "\n",
    "model_test = load_model('./model/1219test.h5')\n",
    "print_score(model_test, X_test,y_test,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [957], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     alive_list, dead_list, alive_num, dead_num \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_patient_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     train_data_id, val_data_id, test_data_id \u001b[38;5;241m=\u001b[39m create_data(alive_list, dead_list)\n\u001b[1;32m     10\u001b[0m     train_x, train_y, val_x, val_y, test_x, test_y \u001b[38;5;241m=\u001b[39m get_data(flag_data_df, data_df,label_df, mode_df,train_data_id, val_data_id, test_data_id)\n",
      "Cell \u001b[0;32mIn [860], line 35\u001b[0m, in \u001b[0;36mcreate_patient_group\u001b[0;34m(label_df, mode_df, kick)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m patient_set:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m id_mode_df \u001b[38;5;241m=\u001b[39m mode_df[\u001b[43mmode_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstay_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstay_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     36\u001b[0m count_complete_mode \u001b[38;5;241m=\u001b[39m id_mode_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mventilator_mode_group\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m12\u001b[39m)\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplete Support\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m group_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(count_complete_mode\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py:6245\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6242\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   6243\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m-> 6245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_result(res_values, name\u001b[39m=\u001b[39;49mres_name)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py:3223\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   3219\u001b[0m     \u001b[39mreturn\u001b[39;00m (res1, res2)\n\u001b[1;32m   3221\u001b[0m \u001b[39m# We do not pass dtype to ensure that the Series constructor\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[39m#  does inference in the case where `result` has object-dtype.\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constructor(result, index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   3224\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n\u001b[1;32m   3226\u001b[0m \u001b[39m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py:484\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_axis(\u001b[39m0\u001b[39;49m, index)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py:593\u001b[0m, in \u001b[0;36mSeries._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39m# The ensure_index call above ensures we have an Index object\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mset_axis(axis, labels)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/pandas/core/internals/managers.py:227\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: \u001b[39mint\u001b[39m, new_labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[39m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_set_axis(axis, new_labels)\n\u001b[1;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis] \u001b[39m=\u001b[39m new_labels\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train = []\n",
    "best_test = []\n",
    "best_val = []\n",
    "best_val_auroc = 0\n",
    "best_test_auroc = 0\n",
    "best_model = model\n",
    "for i in range (100):\n",
    "    alive_list, dead_list, alive_num, dead_num = create_patient_group(label_df,mode_df)\n",
    "    train_data_id, val_data_id, test_data_id = create_data(alive_list, dead_list)\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = get_data(flag_data_df, data_df,label_df, mode_df,train_data_id, val_data_id, test_data_id)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = normalize_data(train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "    for j in range(5):\n",
    "        model = train_NN_module(X_train, y_train, X_val, y_val, epoch=50, learning_rate=0.001, batch=32, verbose=0)\n",
    "        val_auroc = get_score(model, X_val, y_val, 'ss')\n",
    "        test_auroc = get_score(model, X_test, y_test, 'ss')\n",
    "        if(val_auroc+test_auroc>best_val_auroc+best_test_auroc):\n",
    "            best_val_auroc = val_auroc\n",
    "            best_test_auroc = test_auroc\n",
    "            best_model = model\n",
    "            best_train = train_data_id\n",
    "            best_test = val_data_id\n",
    "            best_val = test_data_id\n",
    "            print('========================================')\n",
    "            print_score(model, X_train,y_train,'train')\n",
    "            print_score(model, X_val,y_val,'val')\n",
    "            print_score(model, X_test,y_test,'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_auroc = 0\n",
    "best_test_auroc = 0\n",
    "for j in range(800):\n",
    "    model = train_NN_module(X_train, y_train, X_val, y_val, epoch=45, learning_rate=0.001, batch=32, verbose=0)\n",
    "    val_auroc = get_score(model, X_val, y_val, 'ss')\n",
    "    test_auroc = get_score(model, X_test, y_test, 'ss')\n",
    "    if(val_auroc+test_auroc>best_val_auroc+best_test_auroc):\n",
    "        best_val_auroc = val_auroc\n",
    "        best_test_auroc = test_auroc\n",
    "        best_model = model\n",
    "    if j % 10 == 0:\n",
    "        print(\"best val AUROC:\", best_val_auroc)\n",
    "        print(\"best test AUROC:\", best_test_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========val=========\n",
      "TPR: [0.95535714]\n",
      "TNR: [0.35751295]\n",
      "Accuracy: 73.72%\n",
      "AUROC: 0.7575561312607946\n",
      "=========test=========\n",
      "TPR: [0.97126437]\n",
      "TNR: [0.36]\n",
      "Accuracy: 74.82%\n",
      "AUROC: 0.7697126436781608\n"
     ]
    }
   ],
   "source": [
    "#best_model.save('./group_data/1215best/model.h5')\n",
    "print_score(best_model, X_val,y_val,'val')\n",
    "print_score(best_model, X_test,y_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = 'group_data/1219best/train_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(best_train):\n",
    "        csv_writer.writerow(row)\n",
    "csv_file_name = 'group_data/1219best/val_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(best_val):\n",
    "        csv_writer.writerow(row)\n",
    "csv_file_name = 'group_data/1219best/test_data_id.csv'\n",
    "with open(csv_file_name, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for row in zip(best_test):\n",
    "        csv_writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
